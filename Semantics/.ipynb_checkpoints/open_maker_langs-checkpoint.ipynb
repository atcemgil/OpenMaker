{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery and Representation of Open Making Related Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook sketches the initial exercise on discovering the open making related keywords. The input text is harvested via a Web crawler that identifies and crawls semantically related wikipedia articles.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import tokenizer\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from math import log\n",
    "import json, csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a reference English language corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standard stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/stopwords_standard.txt\", \"r\") as f:\n",
    "    STOP_WORDS_STANDARD = set(f.read().strip().split(\"\\n\"))\n",
    "print(STOP_WORDS_STANDARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Open-making related stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/stopwords_openmaker.txt\", \"r\") as f:\n",
    "    STOP_WORDS_OPENMAKER = set(f.read().strip().split(\"\\n\"))\n",
    "print(STOP_WORDS_OPENMAKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing stop words from the reference English corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merging the two list together\n",
    "STOP_WORDS = STOP_WORDS_STANDARD.union(STOP_WORDS_OPENMAKER)\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load english words from the Brown corpus removing stop words.\n",
    "english_freq_dist = FreqDist([w.lower() for w in nltk.corpus.brown.words()\n",
    "                              if w not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing the rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we remove rare words and get total count. The code below keeps all words with a occurance frequency above 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_freq_dist = {k:v for k,v in english_freq_dist.items() if v > 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading the input Open Maker corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the harvested text from wikipedia.\n",
    "with open(\"data/wikipedia.json\", \"r\") as f: OM_Corpus_text = f.read()\n",
    "OM_Corpus = json.loads(OM_Corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The total number of wiki articles used:\n",
    "print(len(OM_Corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Column names of the the corpus.\n",
    "OM_Corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_articles(tid):\n",
    "    articles = [article for article in OM_Corpus if article['theme.id'] == tid]\n",
    "    for article in articles:\n",
    "        print(article['depth'],article['title'], article['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_articles(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_articles(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_articles(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_articles(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_articles(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_articles(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing a specific corpus based on a theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title(Corpus, theme_id):\n",
    "    title = ''\n",
    "    for article in Corpus:\n",
    "        if article['theme.id'] == theme_id:\n",
    "            title = article['title']\n",
    "            break\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Selecting the specific theme (a sub-corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## For a different sub-corpus use a corresponding theme ID.\n",
    "current_theme_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_title = get_title(OM_Corpus, current_theme_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_fname = \"_\".join([word.capitalize() for word in current_title.split(\" \")])\n",
    "print(current_title, \"::\", output_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that theme.id: 0 corresponds to the the Do IT YOURSELF\n",
    "input_text = \" \".join([page['text'] for page in OM_Corpus if page['theme.id'] == current_theme_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizing the input text:\n",
    "tokenized = tokenizer.tokenize_words(input_text)\n",
    "number_of_words = len(tokenized)\n",
    "print(number_of_words ,current_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Computing frequency distributions of each token, i.e word, term, pancuation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_freq_dist = FreqDist(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Removing punctuation and stopwords from the input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stopword in STOP_WORDS:\n",
    "    if stopword in input_freq_dist:\n",
    "        del input_freq_dist[stopword]\n",
    "        \n",
    "for punctuation in tokenizer.CHARACTERS_TO_SPLIT:\n",
    "    if punctuation in input_freq_dist:\n",
    "        del input_freq_dist[punctuation]\n",
    "\n",
    "# Re-control most common words after cleaning:\n",
    "input_freq_dist.most_common(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Removing rare words from input distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_freq_dist = {k:v for k,v in input_freq_dist.items() if v > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing input vs English corpus volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Total words (after cleaning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = sum(input_freq_dist.values())\n",
    "n_english = sum(english_freq_dist.values())\n",
    "n_input, n_english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Unique words (after cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_unique_word_input = len(input_freq_dist.items())\n",
    "n_unique_word_brown = len(english_freq_dist.items())\n",
    "n_unique_word_input, n_unique_word_brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Cleaned set of input words/terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words in the corpus in case, for a visual inspection. Such inspections will be used both to improve tokenization as well as filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Set of terms/words that occure in both corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words = [w for w in input_freq_dist.keys() & english_freq_dist.keys()]\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in common_words: print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Set of terms/words that occure in the sample but not in the reference corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE EXAMINED: This specific set needs to be incorporated. In fact, it may capture specifity of the content to a great extend. We need to assign a mapping score for each words in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_specifics = dict()\n",
    "for w in input_freq_dist.keys() - english_freq_dist.keys():\n",
    "    input_specifics[w] = input_freq_dist[w]\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(input_specifics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stemming (in case needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for k,v in input_freq_dist.items():\n",
    "    stemmed = stemmer.stem(k)\n",
    "    if stemmed != k: print(k, \"->\", stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Computing representation power of common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine\n",
    "makerness = {}\n",
    "# common_words = [w[0] for w in common_words]\n",
    "for w in common_words:\n",
    "    # Consider only words whose charcater length is larger than 1\n",
    "    if len(w) > 1:\n",
    "        # Log likelihood scores are computed:\n",
    "        score = log((input_freq_dist[w] / n_input) / (english_freq_dist[w] / n_english))\n",
    "        makerness[w] = (score, input_freq_dist[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorting by scores:\n",
    "for k,v in sorted(makerness.items(), key=lambda x:x[1][0], reverse=True): print(v[0],k,v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./output/\"\n",
    "csvfile_name = OUTPUT_FOLDER + \"makerness_\" + output_fname + \".csv\"\n",
    "with open(csvfile_name, 'w') as csvfile:\n",
    "    thewriter = csv.writer(csvfile, delimiter=',')\n",
    "    for k,v in sorted(makerness.items(), key=lambda x:x[1][0], reverse=True):\n",
    "        thewriter.writerow([k,v[0],v[1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(output_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
